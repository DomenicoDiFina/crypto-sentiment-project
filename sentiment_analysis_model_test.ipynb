{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python385jvsc74a57bd0b5e5a555fe56062626c6c405e0840c6a5b32f1baa849bc42cc65cfbb3cd529b7",
      "display_name": "Python 3.8.5 64-bit ('venv': venv)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "source": [
        "Import library"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "from pre_processing import pre_processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "i7mQiVOhV6hF"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /home/s4lv0/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/s4lv0/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Import dataset from: https://www.kaggle.com/kazanova/sentiment140"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('../dataset_sentiment_analysis.csv', names=[\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"tweet\"])"
      ]
    },
    {
      "source": [
        "# View information about dataset"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dim dataset:  1600000\n",
            "Unique sentiment:  [0 4]\n",
            "Number of null elements in columns:\n",
            " sentiment    0\n",
            "id           0\n",
            "date         0\n",
            "query        0\n",
            "user         0\n",
            "tweet        0\n",
            "dtype: int64\n",
            "Percentuage of positive tweets:  50.0 %\n",
            "Percentuage of negative tweets:  50.0 %\n"
          ]
        }
      ],
      "source": [
        "print(\"Dim dataset: \", len(dataset))\n",
        "print(\"Unique sentiment: \", dataset[\"sentiment\"].unique())\n",
        "\n",
        "#check if dataset contain any null values in sentiment and tweet\n",
        "print(\"Number of null elements in columns:\\n\", dataset.isnull().sum())\n",
        "\n",
        "#view number of positive and negative tweets\n",
        "print(\"Percentuage of positive tweets: \", (len(dataset[\"sentiment\"][dataset.sentiment == 4])/len(dataset))*100, \"%\")\n",
        "print(\"Percentuage of negative tweets: \", (len(dataset[\"sentiment\"][dataset.sentiment == 0])/len(dataset))*100, \"%\")"
      ]
    },
    {
      "source": [
        "# Dataset manipulation:\n",
        "- Drop unnecessary information (id, date, query, user)\n",
        "- Replace sentiment id from 0,4 (negative, positive) to 0,1 (negative, positive)\n",
        "- convert column tweet from object to str"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New shape of dataset:  (1600000, 2)\n"
          ]
        }
      ],
      "source": [
        "#shuffle dataset\n",
        "#dataset = dataset.sample(frac=1)\n",
        "#drop unnecessary information\n",
        "dataset.drop([\"id\", \"date\", \"query\", \"user\"], axis=1, inplace=True)\n",
        "#replace sentiment id\n",
        "dataset[\"sentiment\"].replace(4,1)\n",
        "\n",
        "#convert tweet from object to str\n",
        "dataset[\"tweet\"] = dataset[\"tweet\"].astype(\"str\")\n",
        "\n",
        "print(\"New shape of dataset: \", dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing tweets:: 100%|██████████| 1600000/1600000 [01:21<00:00, 19668.84it/s]\n",
            "word tokenize process: 100%|██████████| 1600000/1600000 [02:30<00:00, 10600.72it/s]\n",
            "Remove stop word: 100%|██████████| 1600000/1600000 [02:39<00:00, 10046.46it/s]\n"
          ]
        }
      ],
      "source": [
        "processing_tweet = pre_processing(list(dataset[\"tweet\"]))"
      ]
    },
    {
      "source": [
        "Convert tokenized tweet from list to str"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset[\"processed_tweet\"] = processing_tweet\n",
        "dataset[\"processed_tweet\"] = dataset['processed_tweet'].apply(lambda x: ' '.join(map(str,x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sentiment                                              tweet  \\\n0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n1          0  is upset that he can't update his Facebook by ...   \n2          0  @Kenichan I dived many times for the ball. Man...   \n3          0    my whole body feels itchy and like its on fire    \n4          0  @nationwideclass no, it's not behaving at all....   \n5          0                      @Kwesidei not the whole crew    \n6          0                                        Need a hug    \n7          0  @LOLTrish hey  long time no see! Yes.. Rains a...   \n8          0               @Tatiana_K nope they didn't have it    \n9          0                          @twittera que me muera ?    \n\n                                     processed_tweet  \n0       awww bummer shoulda got david carr third day  \n1  upset update facebook texting might cry result...  \n2  dived many times ball managed save 50 rest go ...  \n3                   whole body feels itchy like fire  \n4                                   behaving mad see  \n5                                         whole crew  \n6                                           need hug  \n7  hey long time see yes rains bit bit laughing l...  \n8                                               nope  \n9                                          que muera  \n"
          ]
        }
      ],
      "source": [
        "print(dataset[:][:10])"
      ]
    },
    {
      "source": [
        "# Save dataset to pickle file\n",
        "### warning: save dataset produce a file dim: ~250mb"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#shuffle dataset\n",
        "dataset = dataset.sample(frac=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#shuffle dataset\n",
        "dataset = dataset.sample(frac=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#shuffle dataset\n",
        "dataset = dataset.sample(frac=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "files = open(\"dataset_sentiment_analysis.pickle\", \"wb\")\n",
        "pickle.dump(dataset, files)\n"
      ]
    },
    {
      "source": [
        "# Creating ML model using LSTM\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.columns\n",
        "\n",
        "tokenizer = Tokenizer(num_words=30000, split=' ')\n",
        "\n",
        "tokenizer.fit_on_texts(dataset['processed_tweet'].values)\n",
        "\n",
        "X = tokenizer.texts_to_sequences(dataset['processed_tweet'])\n",
        "\n",
        "X = pad_sequences(X)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "files = open(\"tokenizer.pickle\", \"wb\")\n",
        "pickle.dump(tokenizer, files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 50, 300)           9000000   \n_________________________________________________________________\nlstm (LSTM)                  (None, 196)               389648    \n_________________________________________________________________\ndense (Dense)                (None, 2)                 394       \n=================================================================\nTotal params: 9,390,042\nTrainable params: 9,390,042\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
          ]
        }
      ],
      "source": [
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(30000, 300,input_length = X.shape[1]))\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(2,activation='sigmoid'))\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='rmsprop',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "Y = pd.get_dummies(dataset['sentiment']).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.20, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1280000, 50) (1280000, 2)\n(320000, 50) (320000, 2)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape,y_train.shape)\n",
        "print(X_test.shape,y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1280000\n"
          ]
        }
      ],
      "source": [
        "print(len(X_train))"
      ]
    },
    {
      "source": [
        "# Check uniformity in splitted dataset"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num neg y_train: 639966\n",
            "num pos y_train: 640034\n",
            "num neg y_test: 160034\n",
            "num pos y_test: 159966\n"
          ]
        }
      ],
      "source": [
        "print(\"num neg y_train:\", [y_train[i][0] for i in range(0, len(y_train))].count(1))\n",
        "print(\"num pos y_train:\", [y_train[i][1] for i in range(0, len(y_train))].count(1))\n",
        "\n",
        "print(\"num neg y_test:\", [y_test[i][0] for i in range(0, len(y_test))].count(1))\n",
        "print(\"num pos y_test:\", [y_test[i][1] for i in range(0, len(y_test))].count(1))"
      ]
    },
    {
      "source": [
        "# Execute this block to split train model"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(0, len(X_train)-320000, 320000):\n",
        "    print(i)\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size = 64, verbose = 1)\n",
        "    model.evaluate(X_test, y_test, verbose=1)\n",
        "    print(\"******* SAVING MODEL *******\")\n",
        "    model.save(\"sentiment_model_lstm\")"
      ]
    },
    {
      "source": [
        "# Execute this block to classic train model"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 1559s 1s/step - loss: 0.4970 - accuracy: 0.7549\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 1551s 1s/step - loss: 0.4399 - accuracy: 0.7930\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 1550s 1s/step - loss: 0.4241 - accuracy: 0.8032\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 1546s 1s/step - loss: 0.4129 - accuracy: 0.8098\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 1548s 1s/step - loss: 0.4012 - accuracy: 0.8168\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 1547s 1s/step - loss: 0.3915 - accuracy: 0.8231\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 1548s 1s/step - loss: 0.3804 - accuracy: 0.8293\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 1549s 1s/step - loss: 0.3696 - accuracy: 0.8354\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 1551s 1s/step - loss: 0.3570 - accuracy: 0.8424\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 1550s 1s/step - loss: 0.3444 - accuracy: 0.8482\n",
            "***** EVALUATION *****\n",
            "10000/10000 [==============================] - 277s 28ms/step - loss: 0.4800 - accuracy: 0.7826\n",
            "INFO:tensorflow:Assets written to: sentiment_model_lstm/assets\n"
          ]
        }
      ],
      "source": [
        "model.fit(X_train, y_train, epochs=10, batch_size=1024, verbose = 1)\n",
        "print(\"***** EVALUATION *****\")\n",
        "model.evaluate(X_test, y_test, verbose=1)\n",
        "model.save(\"sentiment_model_lstm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = load_model(\"sentiment_model_lstm\")"
      ]
    },
    {
      "source": [
        "# KNN Model"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k in range(1,11):\n",
        "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn_model.fit(X_train, y_train)\n",
        "    y_pred = knn_model.predict(X_test)\n",
        "    print(f'k = {k}, accuracy: ', accuracy_score(y_test, y_pred))"
      ]
    }
  ]
}