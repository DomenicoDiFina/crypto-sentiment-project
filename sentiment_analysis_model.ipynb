{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "venv",
      "display_name": "Python 3.8.5 64-bit ('venv': venv)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "source": [
        "Import library"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "from pre_processing import pre_processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "i7mQiVOhV6hF"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "source": [
        "# Import dataset from: https://www.kaggle.com/kazanova/sentiment140"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('../dataset_sentiment_analysis.csv', names=[\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"tweet\"])"
      ]
    },
    {
      "source": [
        "# View information about dataset"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dim dataset:  1600000\n",
            "Unique sentiment:  [0 4]\n",
            "Number of null elements in columns:\n",
            " sentiment    0\n",
            "id           0\n",
            "date         0\n",
            "query        0\n",
            "user         0\n",
            "tweet        0\n",
            "dtype: int64\n",
            "Percentuage of positive tweets:  50.0 %\n",
            "Percentuage of negative tweets:  50.0 %\n"
          ]
        }
      ],
      "source": [
        "print(\"Dim dataset: \", len(dataset))\n",
        "print(\"Unique sentiment: \", dataset[\"sentiment\"].unique())\n",
        "\n",
        "#check if dataset contain any null values in sentiment and tweet\n",
        "print(\"Number of null elements in columns:\\n\", dataset.isnull().sum())\n",
        "\n",
        "#view number of positive and negative tweets\n",
        "print(\"Percentuage of positive tweets: \", (len(dataset[\"sentiment\"][dataset.sentiment == 4])/len(dataset))*100, \"%\")\n",
        "print(\"Percentuage of negative tweets: \", (len(dataset[\"sentiment\"][dataset.sentiment == 0])/len(dataset))*100, \"%\")"
      ]
    },
    {
      "source": [
        "# Dataset manipulation:\n",
        "- Drop unnecessary information (id, date, query, user)\n",
        "- Replace sentiment id from 0,4 (negative, positive) to 0,1 (negative, positive)\n",
        "- convert column tweet from object to str"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New shape of dataset:  (1600000, 2)\n"
          ]
        }
      ],
      "source": [
        "#shuffle dataset\n",
        "#dataset = dataset.sample(frac=1)\n",
        "#drop unnecessary information\n",
        "dataset.drop([\"id\", \"date\", \"query\", \"user\"], axis=1, inplace=True)\n",
        "#replace sentiment id\n",
        "dataset[\"sentiment\"].replace(4,1)\n",
        "\n",
        "#convert tweet from object to str\n",
        "dataset[\"tweet\"] = dataset[\"tweet\"].astype(\"str\")\n",
        "\n",
        "print(\"New shape of dataset: \", dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#too long process (~ 8 hours to process 1.6kkk tweets), using MPI to split processing\n",
        "processing_tweet = pre_processing(list(dataset[\"tweet\"]))"
      ]
    },
    {
      "source": [
        "Suppose that we had split dataset into 4 sub processing: 0-400k, 400-800k, 800k-1.2kk, 1.2kk-1.6kk using pickle library"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_processed = list()\n",
        "for files in [\"../../processing_0_400k.pickle\", \"../../processing_400k_800k.pickle\", \"../../processing_800k_12kk.pickle\", \"../../processing_12kk_end.pickle\"]:\n",
        "    dataset_processed_tmp = pickle.load(open(files, \"rb\"))\n",
        "    for entry in dataset_processed_tmp:\n",
        "        dataset_processed.append(entry)\n",
        "    "
      ]
    },
    {
      "source": [
        "Convert tokenized tweet from list to str"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset[\"processed_tweet\"] = dataset_processed\n",
        "dataset[\"processed_tweet\"] = dataset['processed_tweet'].apply(lambda x: ' '.join(map(str,x)))"
      ]
    },
    {
      "source": [
        "# Save dataset to pickle file\n",
        "### warning: save dataset produce a file dim: ~250mb"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "#shuffle dataset\n",
        "dataset = dataset.sample(frac=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "files = open(\"dataset_sentiment_analysis.pickle\", \"wb\")\n",
        "pickle.dump(dataset, files)\n"
      ]
    },
    {
      "source": [
        "# Creating ML model using LSTM\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.columns\n",
        "\n",
        "tokenizer = Tokenizer(num_words=1500, split=' ')\n",
        "\n",
        "tokenizer.fit_on_texts(dataset['processed_tweet'].values)\n",
        "\n",
        "X = tokenizer.texts_to_sequences(dataset['processed_tweet'])\n",
        "\n",
        "X = pad_sequences(X)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "files = open(\"tokenizer.pickle\", \"wb\")\n",
        "pickle.dump(tokenizer, files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, 48, 300)           600000    \n_________________________________________________________________\nspatial_dropout1d_2 (Spatial (None, 48, 300)           0         \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 196)               389648    \n_________________________________________________________________\ndense_2 (Dense)              (None, 2)                 394       \n=================================================================\nTotal params: 990,042\nTrainable params: 990,042\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
          ]
        }
      ],
      "source": [
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(2000, 300,input_length = X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "Y = pd.get_dummies(dataset['sentiment']).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.30, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(480000, 48) (480000, 2)\n(1120000, 48) (1120000, 2)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape,y_train.shape)\n",
        "print(X_test.shape,y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35000/35000 [==============================] - 4325s 124ms/step - loss: 0.4991 - accuracy: 0.7526\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f203dbf54c0>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "\n",
        "#batch_size = 32\n",
        "model.fit(X_train, y_train, epochs = 1, verbose = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: model_lstm_epoch_1/assets\n"
          ]
        }
      ],
      "source": [
        "model.save(\"model_lstm_epoch_1\")\n",
        "model.save(\"model_lstm_epoch_1.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15000/15000 [==============================] - 397s 26ms/step - loss: 0.4901 - accuracy: 0.7576\n"
          ]
        }
      ],
      "source": [
        "score, acc = model.evaluate(X_test, y_test, verbose=1)"
      ]
    }
  ]
}