{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "venv",
      "display_name": "Python 3.8.5 64-bit ('venv': venv)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "source": [
        "Import library"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "from pre_processing import pre_processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "i7mQiVOhV6hF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "source": [
        "# Import dataset from: https://www.kaggle.com/kazanova/sentiment140"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('../dataset_sentiment_analysis.csv', names=[\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"tweet\"])"
      ]
    },
    {
      "source": [
        "# View information about dataset"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dim dataset:  1600000\n",
            "Unique sentiment:  [0 4]\n",
            "Number of null elements in columns:\n",
            " sentiment    0\n",
            "id           0\n",
            "date         0\n",
            "query        0\n",
            "user         0\n",
            "tweet        0\n",
            "dtype: int64\n",
            "Percentuage of positive tweets:  50.0 %\n",
            "Percentuage of negative tweets:  50.0 %\n"
          ]
        }
      ],
      "source": [
        "print(\"Dim dataset: \", len(dataset))\n",
        "print(\"Unique sentiment: \", dataset[\"sentiment\"].unique())\n",
        "\n",
        "#check if dataset contain any null values in sentiment and tweet\n",
        "print(\"Number of null elements in columns:\\n\", dataset.isnull().sum())\n",
        "\n",
        "#view number of positive and negative tweets\n",
        "print(\"Percentuage of positive tweets: \", (len(dataset[\"sentiment\"][dataset.sentiment == 4])/len(dataset))*100, \"%\")\n",
        "print(\"Percentuage of negative tweets: \", (len(dataset[\"sentiment\"][dataset.sentiment == 0])/len(dataset))*100, \"%\")"
      ]
    },
    {
      "source": [
        "# Dataset manipulation:\n",
        "- Drop unnecessary information (id, date, query, user)\n",
        "- Replace sentiment id from 0,4 (negative, positive) to 0,1 (negative, positive)\n",
        "- convert column tweet from object to str"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['id' 'date' 'query' 'user'] not found in axis\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-0f944204ce10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#dataset = dataset.sample(frac=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#drop unnecessary information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"query\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#replace sentiment id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Desktop/web_data_project/venv/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4306\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4307\u001b[0m         \"\"\"\n\u001b[0;32m-> 4308\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   4309\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4310\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Desktop/web_data_project/venv/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4151\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4153\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Desktop/web_data_project/venv/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   4186\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4187\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4188\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4189\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Desktop/web_data_project/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5589\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5590\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5591\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5592\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5593\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['id' 'date' 'query' 'user'] not found in axis\""
          ]
        }
      ],
      "source": [
        "#shuffle dataset\n",
        "#dataset = dataset.sample(frac=1)\n",
        "#drop unnecessary information\n",
        "dataset.drop([\"id\", \"date\", \"query\", \"user\"], axis=1, inplace=True)\n",
        "#replace sentiment id\n",
        "dataset[\"sentiment\"].replace(4,1)\n",
        "\n",
        "#convert tweet from object to str\n",
        "dataset[\"tweet\"] = dataset[\"tweet\"].astype(\"str\")\n",
        "\n",
        "print(\"New shape of dataset: \", dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#too long process (~ 8 hours to process 1.6kkk tweets), using MPI to split processing\n",
        "processing_tweet = pre_processing(list(dataset[\"tweet\"]))"
      ]
    },
    {
      "source": [
        "Suppose that we had split dataset into 4 sub processing: 0-400k, 400-800k, 800k-1.2kk, 1.2kk-1.6kk using pickle library"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_processed = list()\n",
        "for files in [\"../../processing_0_400k.pickle\", \"../../processing_400k_800k.pickle\", \"../../processing_800k_12kk.pickle\", \"../../processing_12kk_end.pickle\"]:\n",
        "    dataset_processed_tmp = pickle.load(open(files, \"rb\"))\n",
        "    for entry in dataset_processed_tmp:\n",
        "        dataset_processed.append(entry)\n",
        "    "
      ]
    },
    {
      "source": [
        "Convert tokenized tweet from list to str"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset[\"processed_tweet\"] = dataset_processed\n",
        "dataset[\"processed_tweet\"] = dataset['processed_tweet'].apply(lambda x: ' '.join(map(str,x)))"
      ]
    },
    {
      "source": [
        "# Save dataset to pickle file\n",
        "### warning: save dataset produce a file dim: ~250mb"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#shuffle dataset\n",
        "dataset = dataset.sample(frac=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "files = open(\"dataset_sentiment_analysis.pickle\", \"wb\")\n",
        "pickle.dump(dataset, files)\n"
      ]
    },
    {
      "source": [
        "# Creating ML model using LSTM\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.columns\n",
        "\n",
        "tokenizer = Tokenizer(num_words=1500, split=' ')\n",
        "\n",
        "tokenizer.fit_on_texts(dataset['processed_tweet'].values)\n",
        "\n",
        "X = tokenizer.texts_to_sequences(dataset['processed_tweet'])\n",
        "\n",
        "X = pad_sequences(X)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "files = open(\"tokenizer.pickle\", \"wb\")\n",
        "pickle.dump(tokenizer, files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 48, 300)           600000    \n_________________________________________________________________\nspatial_dropout1d (SpatialDr (None, 48, 300)           0         \n_________________________________________________________________\nlstm (LSTM)                  (None, 196)               389648    \n_________________________________________________________________\ndense (Dense)                (None, 2)                 394       \n=================================================================\nTotal params: 990,042\nTrainable params: 990,042\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
          ]
        }
      ],
      "source": [
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(2000, 300,input_length = X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(2,activation='sigmoid'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "Y = pd.get_dummies(dataset['sentiment']).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.30, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1120000, 48) (1120000, 2)\n(480000, 48) (480000, 2)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape,y_train.shape)\n",
        "print(X_test.shape,y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "938/938 [==============================] - 145s 133ms/step - loss: 0.5976 - accuracy: 0.6685\n",
            "Epoch 2/3\n",
            "938/938 [==============================] - 119s 127ms/step - loss: 0.4991 - accuracy: 0.7509\n",
            "Epoch 3/3\n",
            "938/938 [==============================] - 122s 130ms/step - loss: 0.4767 - accuracy: 0.7645\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5e63be7220>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "\n",
        "#batch_size = 32\n",
        "model.fit(X_train[:30000], y_train[:30000], epochs = 3, verbose = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 47s 25ms/step - loss: 0.5307 - accuracy: 0.7335\n",
            "0.5306722521781921 0.7334833145141602\n"
          ]
        }
      ],
      "source": [
        "score, acc = model.evaluate(X_test[:60000], y_test[:60000], verbose=1)\n",
        "print(score, acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "938/938 [==============================] - 120s 128ms/step - loss: 0.5176 - accuracy: 0.7427\n",
            "Epoch 2/3\n",
            "938/938 [==============================] - 121s 129ms/step - loss: 0.4881 - accuracy: 0.7639\n",
            "Epoch 3/3\n",
            "938/938 [==============================] - 122s 130ms/step - loss: 0.4673 - accuracy: 0.7727\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5e6042d790>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "model.fit(X_train[30000:60000], y_train[30000:60000], epochs = 3, verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 47s 25ms/step - loss: 0.5311 - accuracy: 0.7405\n",
            "0.5310587882995605 0.7404500246047974\n"
          ]
        }
      ],
      "source": [
        "score, acc = model.evaluate(X_test[:60000], y_test[:60000], verbose=1)\n",
        "print(score, acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "938/938 [==============================] - 120s 128ms/step - loss: 0.5193 - accuracy: 0.7414\n",
            "Epoch 2/3\n",
            "938/938 [==============================] - 120s 128ms/step - loss: 0.4900 - accuracy: 0.7567\n",
            "Epoch 3/3\n",
            "938/938 [==============================] - 120s 128ms/step - loss: 0.4677 - accuracy: 0.7709\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5e5a2c0fa0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "model.fit(X_train[60000:90000], y_train[60000:90000], epochs = 3, verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 46s 24ms/step - loss: 0.5240 - accuracy: 0.7367\n",
            "0.5239869356155396 0.7367333173751831\n"
          ]
        }
      ],
      "source": [
        "score, acc = model.evaluate(X_test[:60000], y_test[:60000], verbose=1)\n",
        "print(score, acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: model_lstm_epoch_1/assets\n"
          ]
        }
      ],
      "source": [
        "model.save(\"model_lstm_epoch_1\")\n",
        "model.save(\"model_lstm_epoch_1.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "score, acc = model.evaluate(X_test, y_test, verbose=1)"
      ]
    }
  ]
}